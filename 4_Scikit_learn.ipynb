{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFXvywOA1eV9"
      },
      "source": [
        "# scikit-learn 入門/Introduction to scikit-learn\n",
        "\n",
        "* scikit-learn は Python のオープンソース機械学習ライブラリです.\n",
        "\n",
        "* scikit-learn というライブラリを用いて,データを使ってモデルを訓練し,評価するという一連の流れを解説します.\n",
        "\n",
        "* 機械学習の様々な手法を用いる際には,データを使ってモデルを訓練するまでに,以下の **5 つのステップ**がよく共通して現れます.\n",
        "\n",
        "- Step 1：**データセットの準備**\n",
        "- Step 2：**モデルを決める**\n",
        "- Step 3：**目的関数を決める**\n",
        "- Step 4：**最適化手法を選択する**\n",
        "- Step 5：**モデルを訓練する**\n",
        "\n",
        "* これらの 5 つのステップ + テストデータでの精度検証までをscikit-learn の機能を使って簡潔に紹介します.\n",
        "\n",
        "* scikit-learn is an open source machine learning library in Python.\n",
        "\n",
        "* Using the scikit-learn library, we will explain the sequence of steps of training and evaluating models with data.\n",
        "\n",
        "* When using various machine learning methods, the following **five steps** often appear in common before training models with data.\n",
        "\n",
        "- Step 1: **Prepare the dataset**.\n",
        "- Step 2: **Determine the model**.\n",
        "- Step 3: **Determine the objective function**.\n",
        "- Step 4: **Select the optimization method**.\n",
        "- Step 5: **Train the model**.\n",
        "\n",
        "* These 5 steps + up to accuracy validation on test data will be briefly introduced using scikit-learn's functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBsdvRtx1eV_"
      },
      "source": [
        "## scikit-learn を用いた重回帰分析/Multiple regression analysis using scikit-learn\n",
        "\n",
        "* scikit-learn を使ってより大きなデータセットに対し適用してみます.\n",
        "\n",
        "* Apply scikit-learn to a larger data set.\n",
        "\n",
        "### Step 1：データセットの準備/Dataset Preparation\n",
        "\n",
        "* 米国ボストンの 506 の地域ごとの住環境の情報などと家賃の中央値の情報を収集して作られた Boston house prices dataset というデータセットを使用します.\n",
        "\n",
        "* このデータセットには 506 件のサンプルが含まれており,各サンプルは以下の情報を持っています.\n",
        "\n",
        "| 属性名 | 説明 |\n",
        "|:--|:--|\n",
        "| CRIM | 人口 1 人あたりの犯罪発生率 |\n",
        "| ZN | 25,000 平方フィート以上の住宅区画が占める割合 |\n",
        "| INDUS | 小売業以外の商業が占める面積の割合 |\n",
        "| CHAS | チャールズ川の川沿いかどうか (0 or 1) |\n",
        "| NOX | 窒素酸化物の濃度 |\n",
        "| RM | 住居の平均部屋数 |\n",
        "| AGE | 1940 年より前に建てられた持ち主が住んでいる物件の割合 |\n",
        "| DIS | 5 つのボストン雇用施設からの重み付き距離 |\n",
        "| RAD | 環状高速道路へのアクセシビリティ指標 |\n",
        "| TAX | $10,000 あたりの固定資産税率 |\n",
        "| PTRATIO | 町ごとにみた教師 1 人あたりの生徒数 |\n",
        "| B | 町ごとにみた黒人の比率を Bk としたときの (Bk - 0.63)^2 の値 |\n",
        "| LSTAT | 給与の低い職業に従事する人口の割合 |\n",
        "| MEDV | 物件価格の中央値 |\n",
        "\n",
        "* We use a dataset called the Boston house prices dataset, which was created by collecting information on living conditions and median rent for each of 506 neighborhoods in Boston, USA.\n",
        "\n",
        "* The dataset contains 506 samples, each with the following information\n",
        "\n",
        "| Attribute Name | Description |\n",
        "|:--|:--|\n",
        "| CRIM | Crime Rate per Capita |\n",
        "| ZN | Percentage of Residential Parcels Over 25,000 Square Feet|\n",
        "| INDUS | Percentage of area occupied by non-retail commerce |\n",
        "| CHAS | Whether along the Charles River (0 or 1)|\n",
        "| NOX | Concentration of nitrogen oxides|\n",
        "| RM | Average number of rooms in a dwelling |\n",
        "| AGE | Percentage of owner-occupied properties built before 1940|\n",
        "| DIS | Weighted distances from five Boston employment facilities|\n",
        "| RAD | Accessibility Index to Beltway|\n",
        "| TAX | Property tax rate per $10,000|\n",
        "| PTRATIO | Number of students per teacher by town|\n",
        "| B |(Bk - 0.63)^2 where Bk is the percentage of blacks per town\n",
        "| LSTAT | Percentage of population engaged in low-paying occupations\n",
        "| MEDV | median property value\n",
        "\n",
        "* このデータセットを用いて最後の MEDV 以外の 13 個の指標からMEDV を予測する回帰問題に取り組んでみましょう.\n",
        "\n",
        "* このデータセットは, scikit-learn の `load_boston()` という関数を呼び出すことで読み込むことができます.\n",
        "\n",
        "* Using this dataset, let's solve the regression problem of predicting the MEDV from 13 indicators except the last MEDV.\n",
        "\n",
        "* This dataset can be loaded by calling the scikit-learn function `load_boston()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7s7ivnO1eWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "951d3747-76b2-4ead-9cb7-77dfc39e2851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "dataset = load_boston()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxbZdZ6JYqRB"
      },
      "source": [
        "* 読み込んだデータセットは, `data` という属性と `target` という属性を持っており, それぞれに入力値と目標値を並べた ndarray が格納されています.\n",
        "\n",
        "* これらを取り出して,それぞれ `x` と `t` という変数に格納しておきます.\n",
        "\n",
        "* The loaded dataset has an attribute `data` and an attribute `target`, each of which contains an ndarray of input and target values.\n",
        "\n",
        "* Let's take them out and store them in the variables `x` and `t`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a77ezrDIYqRC"
      },
      "outputs": [],
      "source": [
        "x = dataset.data\n",
        "t = dataset.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyXwP_lmYqRD"
      },
      "source": [
        "* 入力値が格納されている `x` は、506 個の 13 次元ベクトルを並べたものになっています.\n",
        "\n",
        "* 形を確認してみます.\n",
        "\n",
        "* The `x` where the input values are stored is a sequence of 506 13-dimensional vectors.\n",
        "\n",
        "* Let's check the shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi4XcqiHYqRD",
        "outputId": "150c16da-5d15-4519-bfe8-2f0f460b11d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viPthHfJYqRE"
      },
      "source": [
        "* 一方 `t` は, 各データ点ごとに 1 つの値を持つため, 506 次元のベクトルになっています.\n",
        "\n",
        "* 形を確認してみます.\n",
        "\n",
        "* On the other hand, `t` has one value for each data point, making it a 506-dimensional vector.\n",
        "\n",
        "* Let's check the shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IbeP7dGYqRE",
        "outputId": "50767042-2a8c-4743-a5a2-13f91ea5a107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "t.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nz_ooJ_YqRF"
      },
      "source": [
        "#### データセットの分割/Splitting a dataset\n",
        "\n",
        "* まずこのデータセットを 2 つに分割します.\n",
        "\n",
        "* それはモデルの訓練に用いるためのデータと, 訓練後のモデルのパフォーマンスをテストするために用いるデータは, 異なるものになっている必要があるためです.\n",
        "\n",
        "* なぜデータセットを分割する必要があるかを説明します.\n",
        "\n",
        "* 例えば, 大学受験の準備のために 10 年分の過去問を購入し, 一部を**勉強のため**に 一部を**勉強の成果をはかる**ために使用したいとします.\n",
        "\n",
        "* 10 年分という限られた数の問題を使って, 結果にある程度の信頼のおけるような方法で実力をチェックするには, 下記の 2 つのうちどちらの方法がより良いでしょうか.\n",
        "\n",
        "  - 10 年分の過去問全てを使って勉強したあと, もう一度同じ問題を使って実力をはかる\n",
        "\n",
        "  - 5 年分の過去問だけを使って勉強し、残りの 5 年分の未だ見たことがない問題を使って実力をはかる\n",
        "\n",
        "* 一度勉強した問題を再び解くことができると確認できても, 大学受験の当日に未知の問題が出たときにどの程度対処できるかを事前にチェックするには不十分です.\n",
        "\n",
        "* よって, 後者のような方法で数限られた問題を活用する方が本当の実力をはかるには有効です.\n",
        "\n",
        "* これは機械学習におけるモデルの訓練と検証でも同様に言えることです.\n",
        "\n",
        "* **実力をつける**ための勉強に使うデータの集まりを**訓練用データセット (training dataset)** といい, **実力をはかる**ために使うデータの集まりを**テスト用データセット (test dataset)** と言います.\n",
        "\n",
        "* このとき, 訓練用データセットとテスト用データセットに含まれるデータの間には**重複がないようにします。**\n",
        "\n",
        "* さきほど用意した `x` と `t` を、訓練用データセットとテスト用データセットに分割します.\n",
        "\n",
        "* どのように分けるかには色々な方法がありますが, 単純に全体の何割かを訓練用データセットとし, 残りをテスト用データセットとする, といった分割を行う方法は**ホールドアウト法 (holdout method)** と呼ばれます.\n",
        "\n",
        "* scikit-learn では, データセットから指定された割合（もしくは個数）のデータをランダムに抽出して訓練用データセットを作成し, 残りをテスト用データセットとする処理を行う関数が提供されています.\n",
        "\n",
        "* First, we split this dataset into two parts.\n",
        "\n",
        "* The reason is that the data used to train the model and the data used to test the performance of the model after training need to be different.\n",
        "\n",
        "* We will explain why we need to split the dataset.\n",
        "\n",
        "* For example, suppose you have purchased 10 years of past papers to prepare for a university exam, and you want to use some of them for **study** and some of them for **measuring the performance of your study**.\n",
        "\n",
        "* Which of the following two methods is a better way to check your performance with a limited number of 10 years' worth of questions in a way that you can have some confidence in your results?\n",
        "\n",
        "  - Study all the past 10 years' worth of problems and then test your ability again with the same problems\n",
        "\n",
        "  - Study only the past 5 years' worth of questions, and then use the remaining 5 years' worth of questions that you have not seen yet to test your ability.\n",
        "\n",
        "* Even if you can confirm that you can solve the problems you have studied once again, it is not enough to check in advance how well you can cope with unknown problems on the day of university entrance examinations.\n",
        "\n",
        "* Therefore, it is more effective to use a limited number of problems in the latter way to measure your real ability.\n",
        "\n",
        "* This is also true for training and validation of models in machine learning.\n",
        "\n",
        "* The collection of data used for **study** is called **training dataset**, and the collection of data used for **testing** is called **test dataset**.\n",
        "\n",
        "* In this case, there should be no **duplication between the data in the training dataset and the test dataset**. \n",
        "\n",
        "* Split the `x` and `t` we have just prepared into a training dataset and a test dataset.\n",
        "\n",
        "* There are many ways to do this, but a simple way is called the **holdout method** where a fraction of the dataset is the training dataset and the rest is the test dataset.\n",
        "\n",
        "* In scikit-learn, a function is provided to randomly extract a specified percentage (or number) of data from a dataset to create a training dataset, and the rest is used as a test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucLKUVrLYqRG"
      },
      "outputs": [],
      "source": [
        "# データセットを分割する関数の読み込み/Loading a function to split a dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 訓練用データセットとテスト用データセットへの分割/Split into training and testing datasets\n",
        "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0msQ40QYqRG"
      },
      "source": [
        "* ここで, `train_test_split()` の `test_size` という引数に 0.3 を与えています.\n",
        "\n",
        "* これはテスト用データセットを全体の 30% のデータを用いて作成することを意味しています.\n",
        "\n",
        "* 自動的に残りの 70% は訓練用データセットとなります.\n",
        "\n",
        "* 上のコードは全サンプルの中から**ランダムに** 70% を訓練データとして抽出し, 残った 30% をテストデータとして返します.\n",
        "\n",
        "* 例えば, データセット中のサンプルが, 目標値が 1 のサンプルが 10 個, 2 のサンプルが 8 個、3 のサンプルが 12個…というように, カテゴリごとにまとめられて並んでいることがあります.\n",
        "\n",
        "* そのとき, データセットの先頭から 18 個目のところで訓練データとテストデータに分割したとすると, 訓練データには目標値が 3 のデータが 1 つも含まれないこととなります.\n",
        "\n",
        "* そこで, ランダムにデータセットを分割する方法が採用されています.\n",
        "\n",
        "* `random_state` という引数に毎回同じ整数を与えることで, 実行のたびに結果が変わることを防いでいます.\n",
        "\n",
        "* それでは, 分割後の訓練データを用いてモデルの訓練、精度の検証を行いましょう.\n",
        "\n",
        "* Here, the `test_size` argument of `train_test_split()` is given as 0.3.\n",
        "\n",
        "* This means that the test dataset is created with 30% of the total data.\n",
        "\n",
        "* Automatically, the remaining 70% will be the training dataset.\n",
        "\n",
        "* The above code will **randomly** extract 70% of all samples as training data and return the remaining 30% as test data.\n",
        "\n",
        "* For example, there may be 10 samples with target value of 1, 8 samples with target value of 2, 12 samples with target value of 3, and so on, grouped by categories.\n",
        "\n",
        "* If you split the dataset into training and test data at the 18th sample from the top, the training data will not contain any data with the target value of 3.\n",
        "\n",
        "* Therefore, a method of random splitting of the dataset is employed.\n",
        "\n",
        "* The same integer is given to the `random_state` argument each time, so that the result will not be different each time you run the simulation.\n",
        "\n",
        "* Now, let's train the model and test the accuracy of the model using the split training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l71nhJfL1eWP"
      },
      "source": [
        "### Step 2 ~ 4：モデル・目的関数・最適化手法を決める/Determine model, objective function, and optimization method\n",
        "\n",
        "* scikit-learn で重回帰分析を行う場合は, `LinearRegression` クラスを使用します.\n",
        "\n",
        "* `sklearn.linear_model` 以下にある `LinearRegression` クラスを読み込んで, インスタンスを作成しましょう.\n",
        "\n",
        "* To perform multiple regression analysis in scikit-learn, use the `LinearRegression` class.\n",
        "\n",
        "* Read the `LinearRegression` class under `sklearn.linear_model` and create an instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLxsQ11I3OG-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# モデルの定義/Define Model\n",
        "reg_model = LinearRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWcEwctR36Dj"
      },
      "source": [
        "* 上記のコードは, 前述の 2 〜 4 までのステップを行います.\n",
        "\n",
        "* `LinearRegression` は最小二乗法を行うクラスで, 目的関数や最適化手法もあらかじめ内部で用意されたものが使用されます.\n",
        "\n",
        "* 詳しくはこちらのドキュメントを参照してください.\n",
        "\n",
        "  - 参考：[sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n",
        "\n",
        "* The above code performs steps 2 through 4 above.\n",
        "\n",
        "* `LinearRegression` is a class that performs the least-squares method, and the objective function and optimization method are prepared in advance.\n",
        "\n",
        "* For details, please refer to this document.\n",
        "\n",
        "  - Reference: [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression .html#sklearn.linear_model.LinearRegression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MtrFyTYYqRI"
      },
      "source": [
        "### Step 5：モデルの訓練/Training of models\n",
        "\n",
        "* 次にモデルの訓練を行います.\n",
        "\n",
        "* scikit-learn に用意されている手法の多くは, 共通して `fit()` というメソッドを持っており, 再利用可能なコードが書きやすくなっています.\n",
        "\n",
        "* `reg_model` を用いて訓練を実行するには, `fit()` の引数に入力値 `x` と目標値 `t` を与えます.\n",
        "\n",
        "* Next, train the model.\n",
        "\n",
        "* Most of the methods provided in scikit-learn have a common method `fit()`, which makes it easy to write reusable code.\n",
        "\n",
        "* To train with `reg_model`, give input value `x` and target value `t` as arguments to `fit()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJbiWfeg3V1L",
        "outputId": "37436190-7912-4e45-82a4-a6f573d061bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# モデルの訓練/Training of models\n",
        "reg_model.fit(x_train, t_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h-Isref4kCB"
      },
      "source": [
        "* モデルの訓練が完了しました.\n",
        "\n",
        "* 求まったパラメータの値を確認してみます.\n",
        "\n",
        "* 重回帰分析では, 重み `w` とバイアス `b` の２つがパラメータでした.\n",
        "\n",
        "* 求まった重み `w` の値は `model.coef_` に、バイアス `b` の値は `model.intercept_` に格納されています.\n",
        "\n",
        "* Training of the model is complete.\n",
        "\n",
        "* Let's check the values of the parameters obtained.\n",
        "\n",
        "* In the multiple regression analysis, the two parameters are the weights `w` and the bias `b`.\n",
        "\n",
        "* The obtained value of the weight `w` is stored in `model.coef_` and the value of the bias `b` is stored in `model.intercept_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_-9poc-3cft",
        "outputId": "4e338e4c-b458-4fca-8920-824a91caec10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.21310401e-01,  4.44664254e-02,  1.13416945e-02,  2.51124642e+00,\n",
              "       -1.62312529e+01,  3.85906801e+00, -9.98516565e-03, -1.50026956e+00,\n",
              "        2.42143466e-01, -1.10716124e-02, -1.01775264e+00,  6.81446545e-03,\n",
              "       -4.86738066e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# 訓練後のパラメータ w/Parameter w after training\n",
        "reg_model.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rozWBiq1eWt",
        "outputId": "af6a11fa-d17d-405d-99fd-6d11e027e2d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37.93710774183309"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 訓練後のバイアス b/Bias b after training\n",
        "reg_model.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQXB57Gr4xd_"
      },
      "source": [
        "* モデルの訓練が完了したら, 精度の検証を行います.\n",
        "\n",
        "* `LinearRegression` クラスは `score()` メソッドを提供しており, 入力値と目標値を与えると訓練済みのモデルを用いて計算した**決定係数 (coefficient of determination)**という指標を返します.\n",
        "\n",
        "* 使用するデータセットのサンプルサイズを $N$, $n$ 個目の入力値に対する予測値を $y_{n}$, 目標値を $t_n$, そしてそのデータセット内の全ての目標値の平均値を $\\bar{t}$ としたとき, 指標は下記のように表される.\n",
        "\n",
        "$$\n",
        "R^{2} = 1 - \\dfrac{\\sum_{n=1}^{N}\\left( t_{n} - y_{n} \\right)^{2}}{\\sum_{n=1}^{N}\\left( t_{n} - \\bar{t} \\right)^{2}}\n",
        "$$\n",
        "\n",
        "* 決定係数の最大値は 1 であり. 値が大きいほどモデルが与えられたデータに当てはまっていることを表します.\n",
        "\n",
        "* Once the model is trained, it is tested for accuracy.\n",
        "\n",
        "* The `LinearRegression` class provides a `score()` method that, given input and target values, returns a **coefficient of determination** calculated using the trained model.\n",
        "\n",
        "* Assuming that the sample size of the dataset used is $N$, the predicted value for the $n$th input value is $y_{n}$, the target value is $t_n$, and the average of all target values in the dataset is $\\bar{t}$, the coefficient is expressed as follows.\n",
        "\n",
        "$$\n",
        "R^{2} = 1 - \\dfrac{\\sum_{n=1}^{N}\\left( t_{n} - y_{n} \\right)^{2}}{\\sum_{n=1}^{N}\\left( t_{n} - \\bar{t} \\right)^{2}}\n",
        "$$\n",
        "\n",
        "* The maximum value of the coefficient of determination is 1. The larger the value, the better the model fits the given data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uNksu9X3pMF",
        "outputId": "f5daa634-3bfc-4735-9d9a-22fd41978cf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7645451026942549"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 精度の検証/Accuracy Verification\n",
        "reg_model.score(x_train, t_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFTlcw5eYqRM"
      },
      "source": [
        "* 訓練済みモデルを用いて訓練用データセットで計算した決定係数は, およそ 0.765 でした.\n",
        "\n",
        "* The coefficient of determination calculated on the training dataset using the trained model was approximately 0.765."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndGdp3kB4ZPZ"
      },
      "source": [
        "### 新しい入力値に対する予測の計算（推論）/Computation of predictions for new input values (inference)\n",
        "\n",
        "* 訓練が終わったモデルに, 新たな入力値を与えて予測値を計算させるには, `predict()` メソッドを用います.\n",
        "\n",
        "* 訓練済みのモデルを使ったこのような計算は、**推論 (inference)** と呼ばれることがあります.\n",
        "\n",
        "* 今回は, 訓練済みモデル `reg_model` を用いて, テスト用データセットからサンプルを 1 つ取り出し, 推論を行ってみます.\n",
        "\n",
        "* このとき`predict()` メソッドに与える入力値の ndarray の形が `(サンプルサイズ, 各サンプルの次元数)` となっている必要があることに気をつけてください.\n",
        "\n",
        "* To compute predictions for a trained model given new input values, use the method `predict()`.\n",
        "\n",
        "* Such a computation with a trained model is sometimes called **inference**.\n",
        "\n",
        "* In this case, we will use a trained model `reg_model` to perform inference on a sample from a test dataset.\n",
        "\n",
        "* Note that the ndarray of input values to the `predict()` method must be in the shape of `(sample size, number of dimensions of each sample)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e84j6bbh51Wg",
        "outputId": "68e9cb61-3098-42aa-86d1-b3e729b66c98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24.9357079])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "reg_model.predict(x_test[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v0gGYECYqRN"
      },
      "source": [
        "* この入力に対する目標値を見てみます.\n",
        "\n",
        "* Let's look at the target value for this input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZQK1L9iYqRN",
        "outputId": "8186268f-9775-409d-ff1f-a88af15371c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.6"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "t_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idv6jkwnYqRN"
      },
      "source": [
        "* 22.6 という目標値に対して, およそ 24.94 という予測値が返ってきました.\n",
        "\n",
        "* The target value of 22.6 was returned as a prediction of approximately 24.94."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3d1KBLlYqRN"
      },
      "source": [
        "### テスト用データセットを用いた評価/Evaluation using test datasets\n",
        "\n",
        "* 訓練済みモデルの性能をテスト用データセットを使って決定係数を計算することで評価してみます.\n",
        "\n",
        "* Evaluate the performance of the trained model by computing the coefficient of determination using a test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSiL_fmE56W4",
        "outputId": "94501b8b-73bc-48f7-c354-752970304344"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6733825506400171"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "reg_model.score(x_test, t_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DedG5oFGYqRO"
      },
      "source": [
        "* 訓練用データセットを用いて算出した値（およそ 0.765）よりも、低い値がでてしまいました.\n",
        "\n",
        "* 教師あり学習の目的は, 訓練時には見たことがない新しいデータ, ここではテスト用データセットに含まれているデータに対しても高い性能を発揮するように, モデルを訓練することです.\n",
        "\n",
        "* 逆に, 訓練時に用いたデータに対してはよく当てはまっていても, 訓練時に用いなかったデータに対しては予測値と目標値の差異が大きくなってしまう現象を**過学習 (overfitting)** と言います.\n",
        "\n",
        "* 過学習を防ぐために, 色々な方法が研究されています.\n",
        "\n",
        "* ここでは, データに前処理を行い, テスト用データセットを用いて計算した決定係数を改善します.\n",
        "\n",
        "* This value is lower than the value calculated using the training dataset (approximately 0.765).\n",
        "\n",
        "* The purpose of supervised learning is to train the model to perform well on new data that has not been seen during training, in this case data contained in the test dataset.\n",
        "\n",
        "* Conversely, **overfitting** is a phenomenon in which the model performs well on the data used in training, but the difference between the predictions and the target values becomes large on the data not used in training.\n",
        "\n",
        "* Various methods have been studied to prevent overfitting.\n",
        "\n",
        "* Here, we pre-process the data and improve the coefficients of determination computed on a test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMot2VaYqRO"
      },
      "source": [
        "## 各ステップの改善/Improvement of each step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwpv2FxdFSYi"
      },
      "source": [
        "### Step 1 の改善：前処理/Step 1 Improvement: preprocessing\n",
        "\n",
        "* **前処理 (preprocessing)** とは, 欠損値の補完, 外れ値の除去, 特徴量選択, 正規化などの処理を訓練を開始する前にデータセットに対して行うことです.\n",
        "\n",
        "* 手法やデータに合わせた前処理が必要となるため, 適切な前処理を行うためには手法そのものについて理解している必要があるだけでなく, 使用するデータの特性についてもよく調べておく必要があります.\n",
        "\n",
        "* 今回のデータは, 入力値の値の範囲が CRIM, ZN, INDUS, ... といった指標ごとに大きく異なっています.\n",
        "\n",
        "* そこで, 各入力変数ごとに平均が 0, 分散が 1 となるように値をスケーリングする**標準化 (standardization)** をおこなってみます.\n",
        "\n",
        "* scikit-learn では `sklearn.preprocessing` というモジュール以下に `StandardScaler` というクラスが定義されています.\n",
        "\n",
        "* 今回は, これを用いてデータセットに対し標準化を適用します.\n",
        "\n",
        "* `StandardScaler` クラスを読み込み, インスタンスを作成します。\n",
        "\n",
        "* **preprocessing** is the process of performing missing value completion, outlier removal, feature selection, normalization, etc. on a dataset before starting the training.\n",
        "\n",
        "* Since the preprocessing must be tailored to the method and the data, in order to perform appropriate preprocessing, it is necessary to have a good understanding of the method itself as well as the characteristics of the data to be used.\n",
        "\n",
        "* The range of values of input variables is very different for each indicator, such as CRIM, ZN, INDUS, ....\n",
        "\n",
        "* Therefore, we will perform a **standardization** for each input variable, scaling the values so that the mean is 0 and the variance is 1.\n",
        "\n",
        "* In scikit-learn, the class `StandardScaler` is defined under the module `sklearn.preprocessing`.\n",
        "\n",
        "* This time, we will use this class to apply standardization to a dataset.\n",
        "\n",
        "* Load the `StandardScaler` class and create an instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLQNMom0CZXA"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_CsbDW_1eX4"
      },
      "source": [
        "* 標準化を行うためには, データセットの各入力変数ごとの平均と分散の値を計算する必要があります.\n",
        "\n",
        "* この計算は, `scaler` オブジェクトがもつ `fit()` メソッドを用いて行います.\n",
        "\n",
        "* 引数には, 平均・分散を計算したい入力値の ndarray を渡します.\n",
        "\n",
        "* In order to perform standardization, we need to compute the mean and variance for each input variable of the dataset.\n",
        "\n",
        "* This calculation is done using the `fit()` method of the `scaler` object.\n",
        "\n",
        "* The argument is an ndarray of the input values for which you want to compute the mean and * variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YROpwqRhCZca",
        "outputId": "1cf1d583-7d9f-4ab5-8304-8b1ee9a66e79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "scaler.fit(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmJsOAbP1eX7"
      },
      "source": [
        "* すべてのサンプルではなく, 訓練用データセットのみを用いてこれらの値を算出します.\n",
        "\n",
        "* 先ほどの `fit()` の実行の結果算出された平均値が `mean_` 属性に, 分散が `var_` 属性に格納されているので, 確認してみます.\n",
        "\n",
        "* We calculate these values using only the training dataset, not the whole sample.\n",
        "\n",
        "* Let's check the mean and variance calculated from the previous run of `fit()`, which are stored in the attribute `mean_` and `var_`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbWSTWHB1eX8",
        "outputId": "adc5f605-67e3-4b8c-a4d9-c720798bb745"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.35828432e+00, 1.18093220e+01, 1.10787571e+01, 6.49717514e-02,\n",
              "       5.56098305e-01, 6.30842655e+00, 6.89940678e+01, 3.76245876e+00,\n",
              "       9.35310734e+00, 4.01782486e+02, 1.84734463e+01, 3.60601186e+02,\n",
              "       1.24406497e+01])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# 平均/mean\n",
        "scaler.mean_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAXIw1831eX9",
        "outputId": "f673f3ce-48bb-4dde-bf8e-9222c6ba2c82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6.95792305e+01, 5.57886665e+02, 4.87753572e+01, 6.07504229e-02,\n",
              "       1.33257561e-02, 4.91423928e-01, 7.83932705e+02, 4.26314655e+00,\n",
              "       7.49911344e+01, 2.90195600e+04, 4.93579208e+00, 7.31040807e+03,\n",
              "       4.99634123e+01])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# 分散/variance\n",
        "scaler.var_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYZml5H31eX-"
      },
      "source": [
        "* これらの平均・分散の値を使ってデータセットに含まれる値に標準化を施すには, `transform()` メソッドを使用します.\n",
        "\n",
        "* To standardize the values in the dataset using these mean and variance values, use the method `transform()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sz88ORaLw3_"
      },
      "outputs": [],
      "source": [
        "x_train_scaled = scaler.transform(x_train)\n",
        "x_test_scaled  = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzjItBITYqRP"
      },
      "source": [
        "*標準化を行ったデータを使って同じモデルを訓練してみます.\n",
        "\n",
        "*Let's train the same model using the standardized data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdGJ0lNnLxJY",
        "outputId": "16a36e9c-9362-45ba-8857-c30c1c0bce48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "reg_model = LinearRegression()\n",
        "\n",
        "# モデルの訓練/Training of models\n",
        "reg_model.fit(x_train_scaled, t_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70X7rqb3LxW3",
        "outputId": "3f81fc65-1d53-466d-8d3a-596c0e18f8fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7645451026942549"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# 精度の検証（訓練データ）/Verification of accuracy (training data)\n",
        "reg_model.score(x_train_scaled, t_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd1uZ0_Y1eYE",
        "outputId": "8476409e-99c0-4d56-8877-61ada709a978"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6733825506400195"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 精度の検証（テストデータ）/Verification of accuracy (test data)\n",
        "reg_model.score(x_test_scaled, t_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raWt9o5FYqRQ"
      },
      "source": [
        "* 結果は変わりませんでした.\n",
        "\n",
        "* べき変換をする別の前処理を適用し, 再度同じモデルの訓練を行ってみます.\n",
        "\n",
        "* The result did not change.\n",
        "\n",
        "* We try to train the same model again, applying another preprocessing which performs a power law transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTNnBzuiYqRQ",
        "outputId": "912120fb-9de9-42f1-c5db-c39373ba7436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
            "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "scaler = PowerTransformer()\n",
        "scaler.fit(x_train)\n",
        "\n",
        "x_train_scaled = scaler.transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(x_train_scaled, t_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVICHk1dYqRQ",
        "outputId": "448d6a4b-2ece-45a2-b6f6-804c9f1a91dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7859862563286062"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# 訓練データでの決定係数/Coefficient of determination on training data\n",
        "reg_model.score(x_train_scaled, t_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ptH8SjaYqRR",
        "outputId": "572072f6-db27-4b87-ec6f-70c1bdecf736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7002856551689584"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# テストデータでの決定係数/Coefficient of determination on test data\n",
        "reg_model.score(x_test_scaled, t_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rkchSc_1eYG"
      },
      "source": [
        "* 結果が改善しました.\n",
        "\n",
        "* Results have improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNERbB_21eYG"
      },
      "source": [
        "#### パイプライン化/pipelining\n",
        "\n",
        "* 前処理用の `scaler` と 重回帰分析を行う `reg_model` は, 両方 `fit()` メソッドを持っていました.\n",
        "\n",
        "* scikit-learn には, パイプラインと呼ばれる一連の処理を統合できる機能があります。\n",
        "\n",
        "* これを用いてこれらの処理をまとめてみます.\n",
        "\n",
        "* Both `scaler` for preprocessing and `reg_model` for multiple regression analysis had a `fit()` method.\n",
        "\n",
        "* scikit-learn has a feature called pipeline that allows you to integrate a series of processes.\n",
        "\n",
        "* Use this to put these processes together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qudHL8J51eYG"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# パイプラインの作成 (scaler -> svr)/Create pipeline (scaler -> svr)\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', PowerTransformer()),\n",
        "    ('reg', LinearRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmpM51eh1eYK",
        "outputId": "51d71a13-03bb-4d41-b0c0-c4b2084bff86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
            "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('scaler', PowerTransformer()), ('reg', LinearRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# scaler および reg を順番に使用/Use scaler and reg in sequence\n",
        "pipeline.fit(x_train, t_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz8N8fkf1eYM",
        "outputId": "b3d29daa-4146-4d97-fec8-10721b7dbe5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7859862563286062"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# 訓練用データセットを用いた決定係数の算出/Calculation of coefficient of determination using training dataset\n",
        "pipeline.score(x_train, t_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebbWBAC31eYN",
        "outputId": "065be5ff-4a97-4d6a-9323-93c18b12437a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7002856551689584"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# テスト用データセットを用いた決定係数の算出/Calculation of coefficients of determination using test dataset\n",
        "linear_result = pipeline.score(x_test, t_test)\n",
        "\n",
        "linear_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSPXVQxK1eYO"
      },
      "source": [
        "* パイプライン化を行うことで, `x_train_scaled` のような中間変数を作成することなく, 同じ処理が行えるようになりました.\n",
        "\n",
        "* これによってコード量が減らせるだけでなく, 評価を行う前にテスト用データセットに対しても訓練用データセットに対して行ったのと同様の前処理を行うことを忘れてしまうといったミスを防ぐことができます.\n",
        "\n",
        "* Pipelining allows us to do the same without creating intermediate variables like `x_train_scaled`.\n",
        "\n",
        "* This not only reduces the amount of code, but also avoids the mistake of forgetting to do the same preprocessing on the test dataset before evaluation as we did on the training dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "4_Scikit_learn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}